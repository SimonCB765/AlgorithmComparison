The following assumptions are made in these instructions:
	PISCES code is located at path\to\PISCES\bin
	Python code is located at path\to\python\code

###########################################################
BHOSLIB
###########################################################
Directory Structure Expected
-----------------------------------------------------------
The expected structure is a directory containing all the unzipped BHOSLIB graphs.
An example of the expected structure is:
	path\to\BHOSLIB
		\frb30-15-1.mis
		\frb30-15-2.mis
		...
		\frb100-40.mis

Directory Structure Generated
-----------------------------------------------------------
Assuming that Leaf and GLP are the only two algorithms used, an example of the expected structure is:
	path\to\BHOSLIB
		\frb30-15-1.mis
		\frb30-15-1-GLP.txt
		\frb30-15-1-Histogram.txt
		\frb30-15-1-Leaf.txt
		\frb30-15-2.mis
		\frb30-15-2-GLP.txt
		\frb30-15-2-Histogram.txt
		\frb30-15-2-Leaf.txt
		...
		\frb100-40.mis
		\frb100-40-GLP.txt
		\frb100-40-Histogram.txt
		\frb100-40-Leaf.txt
		\Results.txt
path\to\BHOSLIB\frbXXX-GLP.txt contains the nodes removed from graph frbXXX by GLP.
path\to\BHOSLIB\frbXXX-Histogram.txt contains data on the degree of each node in graph frbXXX, and how often the different degrees occur.
path\to\BHOSLIB\frbXXX-Leaf.txt contains the nodes removed from graph frbXXX by Leaf.
path\to\BHOSLIB\Results.txt contains a table of the results of all algorithms run on all the BHOSLIB graphs.

Workflow
-----------------------------------------------------------
1) Download the BHOSLIB graphs from http://www.nlsde.buaa.edu.cn/~kexu/benchmarks/graph-benchmarks.htm, and unzip them.
2) Set up the directory strucutre as expected.
3) Generate the BHOSLIB algorithm comparison results (ensure that the time limit is properly set within the code).
	Assuming that the top level directory is called path\to\BHOSLIB, an example of the command used is as follows:
		python path\to\python\code\controllerBHOSLIB.py path\to\BHOSLIB Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1

###########################################################
Biological Datasets
###########################################################
Directory Structure Expected (by step 4)
-----------------------------------------------------------
The same as the Protein Subset Datasets. The only difference is that there will be one dataset sub-directory. An example is as follows:
	\path\to\results\directory
		\BioDataset
			\BioDataset.fasta
			\Aligned.txt
Note that the name of the dataset sub-directory (e.g. DatasetN) is the same as the name of the FASTA file of the dataset (e.g. DatasetN.fasta).

Directory Structure Generated (by step 4)
-----------------------------------------------------------
The same as the Protein Subset Datasets. The only difference is that there will be one dataset sub-directory. An example is as follows:
	\path\to\results\directory
		\BioDataset
			\XDegreeHistogram.txt
			\XLeafCull.txt
			\XPISCESCull.txt
			\YDegreeHistogram.txt
			\YLeafCull.txt
			\YPISCESCull.txt
			\BioDataset.fasta
			\Aligned.txt
		\XPercentResults.txt
		\YPercentResults.txt
The ZPercentResults.txt files containn a table of the results of performing the culling with a Z% cut-off on the dataset.

Workflow
-----------------------------------------------------------
Although this method was used for specific biological datasets (e.g. human proteome, mouse proteome etc.), it can be used for any collection of proteins.

1) Aquire the FASTA format file of the biological dataset to use.
2) BLAST the file and create the file of alignments using PISCES.
	This uses the modified Cull_for_UserSEQ.pl PISCES file. Perform this step using the command:
		perl path\to\PISCES\bin\Cull_for_UserSEQ.pl -i path\to\FASTA\file
	As an example, assume that the FASTA file is called Human.fasta, and that the file is located at C:\Fasta\Human.fasta.
	Then the path\to\FASTA\file used would be C:\Fasta\Human, the '.fasta' at the end is removed. For any FASTA file submitted you should remove the file extension, irrespective of the actual extension.
3) After generating the PISCES alignment file, go through the alignment file and change all [0-9]-[0-9] regular expression matches to [0-9] [0-9] (i.e. replace the '-' with a ' ').
	DO NOT just change all '-' to ' ', as this will mess up the e value column (i.e. the 4.7e-002 bit).
	This needs to be done because PISCES sometimes writes out alignments like this:
		sp|Q8WZ42     34 11669   370-12005  sp|A6NNJ1    464     4   371    89    15  4.7e-002   388
	The error is in the 370-12005 part. To fix this (in order that PISCES culling works properly) replace the '-' with a ' '.
	PISCES will work even if the replacement is not made. However, there will be redundancy left in the 'non-redundant' dataset.
4) Rename the pdbaa.align file created in the PISCES bin directory to Aligned.txt.
	Move the Aligned.txt file and the C:\Fasta\Human.fasta file to the same directory. This directory must not be the PISCES bin directory.
5) Run the algorithm comparison on the dataset (ensure that the time limit is properly set within the code).
	An example of the command used for this is:
		python path\to\python\code\controllersubsets.py path\to\results\directory Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1

###########################################################
Exact MIS
###########################################################
Directory Structure Expected
-----------------------------------------------------------
The structure expected by the process is as follows:
	path\to\Results
		\Results100
		\Results200
		\Results500
		\Results1000
		\Results2000
		\Results5000
Each of the \Results sub-directories is generated by step 5 of the Protein Subset Datasets workflow (i.e. they are eqivalent to the \path\to\results\directory\subdir in the structure in that process).
For example, \Results100 results from the command:
	python path\to\python\code\controllersubsets.py path\to\Results\Results100 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1

Directory Structure Generated
-----------------------------------------------------------
The results directory structure is separate from the input structure. The layout is as follows:
	path\to\Results\ComparisonResults
		\100
			\100-20.txt
			\100-30.txt
			\100-40.txt
			\100-50.txt
			\100-60.txt
			\100-70.txt
			\100-80.txt
			\100-90.txt
		\200
		\500
		\1000
		\2000
		\5000
The \200, \500, \1000, \2000 and \5000 sub-directories all contain files named in the same manner as the \100 sub-directory files (i.e. \200 contains 200-XX.txt files, \500 has 500-XX.txt etc.).
Only files in the \100, \200, \500 and \1000 sub-directories contain actual results. This is because there were no exact results generated for datasets of 2000 and 5000 proteins.

Workflow
-----------------------------------------------------------
1) Generate the 6 expected directories.
	This involves running the six commands:
		python path\to\python\code\controllersubsets.py path\to\Results\Results100 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
		python path\to\python\code\controllersubsets.py path\to\Results\Results200 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
		python path\to\python\code\controllersubsets.py path\to\Results\Results500 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
		python path\to\python\code\controllersubsets.py path\to\Results\Results1000 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
		python path\to\python\code\controllersubsets.py path\to\Results\Results2000 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
		python path\to\python\code\controllersubsets.py path\to\Results\Results5000 Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
	The process for this is outlined in the Protein Subset Datasets workflow.
2) Get access to the exact (Cliquer) and GLP results in the expected CSV format, and put them in the locations expected by the exact commparison script.
	The format of the CSV files is:
		dataset name, % cutoff, # isolated nodes, size of MIS, time taken
3) Run the exact comparison script.
	The command for this is:
		python path\to\python\code\comparewithexactMIS.py

###########################################################
GLP Leaf Long Run Comparison
###########################################################
Directory Structure Expected
-----------------------------------------------------------
Identical to the Protein Subset Datasets Directory Structure Expected section.

Directory Structure Generated
-----------------------------------------------------------
Identical to the Protein Subset Datasets Directory Structure Generated section.

Workflow
-----------------------------------------------------------
If the datasets have already been generated, then skip to step 5.
If the Protein Subset Datasets workflow has already been carried out, then the datasets generated in that workflow can be used for this one.
	However, the directory with the results of that workflow should be copied somewhere else to avoid overwriting the results of that workflow.

1) Identical to the Protein Subset Datasets workflow step 1.
2) Identical to the Protein Subset Datasets workflow step 2.
3) Identical to the Protein Subset Datasets workflow step 3.
4) Generate the subsets of the Human.fasta file.
	The command used for this was:
		python path\to\python\code\alignmentscreatefiles.py C:\Fasta\Human.fasta path\to\Aligned.txt 5000 50 path\to\results\directory
	This will generate 50 subsets of Human.fasta, each of 5000 sequences, in the directory path\to\results\directory\Results5000.
	Refer to the Directory Structure Expected section of the Protein Subset Datasets section for information on the directory structure this command generates.

At this point the datasets have all been generated, and the comparisons between the algorithms can be run.

5) Run the algorithm comparison on the datasets generated (ensure that the time limit is properly set within the code).
	The command used for this was:
		python path\to\python\code\controllersubsets.py path\to\results\directory\Results5000 Leaf-GLP 1
	Other algorithms can be used, but unless they are timee bounded there is not much point.

###########################################################
PDB
###########################################################
Directory Structure Expected
-----------------------------------------------------------
There are two directory structures expected. The first is the BLASTDB directory. This should take the form of:
path\to\BLAST\directory
	\MakeNonRedundant.log.pdb
	\resolution.dat
	\pdbaa
	\pdbaa.align
This directory can be set up simply by downloading and unzipping the BLASTDB.tar.gz directory from PISCES (http://dunbrack.fccc.edu/Guoli/pisces_download.php#BLASTDB).

The seecond directory contains the gzipped files of the PISCES generated culled lists. The structure should be:
path\to\culled\lists
	\cullpdb_pc20_res1.6_R0.25_d120828_chains2068.gz
	\cullpdb_pc20_res1.8_R0.25_d120828_chains3250.gz
	...
	\cullpdb_pc90_res100_R100_inclNOTXRAY_inclCA_d120828_chains28381.gz

Directory Structure Generated
-----------------------------------------------------------
The BLASTDB directory structure is not changed by the comparison script.
The directory of culled lists is only changed by the addition of a results file, as follows:
path\to\culled\lists
	\cullpdb_pc20_res1.6_R0.25_d120828_chains2068.gz
	\cullpdb_pc20_res1.8_R0.25_d120828_chains3250.gz
	...
	\cullpdb_pc90_res100_R100_inclNOTXRAY_inclCA_d120828_chains28381.gz
	\Results.txt


Workflow
-----------------------------------------------------------
1) Download the pre-compiled culled lists from PISCES (http://dunbrack.fccc.edu/Guoli/pisces_download.php#culledpdb).
2) Download the BLASTDB.tar.gz directory from PISCES (http://dunbrack.fccc.edu/Guoli/pisces_download.php#BLASTDB).
3) Run the PISCES comparison.
	The command used for this was:
		python path\to\python\code\controllerPDB.py path\to\BLASTDB path\to\PreCompiled
	path\to\BLASTDB is the location of the unzipped BLASTDB directory (from step 2).
	path\to\PreCompiled is the location of the pre-compiled lists (from step 1).

###########################################################
Protein Length
###########################################################
Directory Structure Expected
-----------------------------------------------------------
The directory structure expected is the same as the directory structure generated by the Protein Subset Datasets or Biological Datasets workflow.
As an example, a basic format for the strucutre expected is:
	path\to\results\directory
		\Dataset0
		...
		\DatasetN
		\XPercentResults.txt
		\YPercentResults.txt
		\ZPercentResults.txt

Directory Structure Generated
-----------------------------------------------------------
The directory structure is augmented by adding the protein length data.
The new structure is:
	path\to\results\directory
		\Dataset0
		...
		\DatasetN
		\XHistogramData.txt
		\XPercentResults.txt
		\YHistogramData.txt
		\YPercentResults.txt
		\ZHistogramData.txt
		\ZPercentResults.txt
The HistogramData.txt files will each contain a table of algorithms and the lengths of the proteins kept by them. There will also be a column for the redundant dataset.


Workflow
-----------------------------------------------------------
1) Go through the workflow for either the Protein Subset Datasets or the Biological Datasets.
	The choice depends on whether the dataset you are generating length data for is a subset or an entire proteome.
2) Generate the length data.
	An example of the type of command used for this is:
		python path\to\python\code\proteinlengthhistogramgenerator.py path\to\results\directory DatasetName AlgsToUse
	path\to\results\directory is the path to the input directory (the one containing the results of the Protein Subset Datasets or Biological Datasets workflow).
	DatasetName is the name of the sub-directory (dataset) in the path\to\results\directory directory. This is also the name of the FASTA format file for the dataset.
	AlgsToUse is thelist of the algorithms must only contain algorithms used in either the Protein Subset Datasets or the Biological Datasets algorithm comparison.

###########################################################
Protein Non-redundant Datasets
###########################################################
Directory Structure Expected
-----------------------------------------------------------
The directory structure expected is the same as the directory structure generated by the Protein Subset Datasets or Biological Datasets workflow.
As an example, a basic format for the strucutre expected is:
	path\to\results\directory
		\Dataset0
		...
		\DatasetN
		\XPercentResults.txt
		\YPercentResults.txt
		\ZPercentResults.txt

Directory Structure Generated
-----------------------------------------------------------
The directory generated contains the non-redundant datasets of the specified algorithms at all the percentage sequence identities run.
The name of the output files will be of the form: Alg-Dataset-Seqid.txt
	Alg will be the name of the algorithm (Leaf, PISCES, BlastCuller etc.)
	Dataset will be the name of the dataset (HumanProteome, 5000-00, 250-13 etc.)
	Seqid will be the percentage sequence identity (10, 20, 25 etc.)
An example of the output directory structure would be:
	path\to\output\directory
		\Leaf-Human-10.txt
		\Leaf-Human-20.txt
		...
		\Leaf-Human-90.txt
		\PISCES-Human-10.txt
		...
		\PISCES-Human-90.txt

Workflow
-----------------------------------------------------------
1) Go through the workflow for either the Protein Subset Datasets or the Biological Datasets.
	The choice depends on whether the non-redundant dataset you are determining is for a subset or an entire proteome.
2) Generate the files of the proteins in the non-redundant datasets.
	An example of the type of command used for this is:
		python path\to\python\code\determinenonredundantproteins.py path\to\results\directory AlgsToUse path\to\output\directory
	path\to\results\directory is the path to the input directory (the one containing the results of the Protein Subset Datasets or Biological Datasets workflow).
	AlgsToUse is thelist of the algorithms must only contain algorithms used in either the Protein Subset Datasets or the Biological Datasets algorithm comparison.
	path\to\output\directory is the path to the directory that will contain the output results of the deficiency checking.

###########################################################
Protein Subset Datasets
###########################################################
Directory Structure Expected (generated by step 4 and used by step 5)
-----------------------------------------------------------
For each call of controllersubsets.py, the expected strucutre of the input path\to\results\directory\subdir directory is as follows:
	path\to\results\directory\subdir
		\Dataset0
			\Dataset0.fasta
			\Aligned.txt
		\Dataset1
			\Dataset1.fasta
			\Aligned.txt
		...
		\DatasetN
			\DatasetN.fasta
			\Aligned.txt
Note that the name of the dataset sub-directory (e.g. DatasetN) is the same as the name of the FASTA file of the dataset (e.g. DatasetN.fasta).
Each Aligned.txt is the alignment file for the proteins in the dataset named by the directory that contains it.
For example, path\to\results\directory\subdir\Dataset0\Aligned.txt contains the alignments for the proteins in the dataset path\to\results\directory\subdir\Dataset0\Dataset0.fasta.
There is one sub-directory under path\to\results\directory\subdir for every dataset that the algorithms will be compared on.

Directory Structure Generated (by step 5)
-----------------------------------------------------------
The directory structure generated is the same as the expected structure (as the same directory is used for input and output).
The only additions are the results files generated by each algorithm, and the overall results files for each percentage similarity cut off.
In each path\to\results\directory\subdir\DatasetX directory, the results of culling using each algorithm at each percentage cut off are added, along with a histogram of the degree of the nodes in the graph at each percentage cut off.
In addition, one file for each percentage cut off is added under the path\to\results\directory\subdir directory. These files are named path\to\results\directory\subdir\XPercentResults.txt, where X is the percentage cut off used.
An example strucutre would be:
	\path\to\results\directory\subdir
		\Dataset0
			\20DegreeHistogram.txt
			\20LeafCull.txt
			\20PISCESCull.txt
			\Dataset0.fasta
			\Aligned.txt
		...
		\DatasetN
			\20DegreeHistogram.txt
			\20LeafCull.txt
			\20PISCESCull.txt
			\DatasetN.fasta
			\Aligned.txt
		\20PercentResults.txt
path\to\results\directory\subdir\Dataset0\20DegreeHistogram.txt contains the edge histogram data for the 20% similarity cut off.
path\to\results\directory\subdir\Dataset0\20LeafCull.txt contains the names of the proteins removed by the Leaf algorithm with a 20% similarity cut off.
path\to\results\directory\subdir\Dataset0\20PISCESCull.txt contains the names of the proteins removed by the PISCES algorithm with a 20% similarity cut off.
path\to\results\directory\subdir\20PercentResults.txt is a tab delimited file of the results for all N dataset and all algorithms used at 20% similarity cut off.

Workflow
-----------------------------------------------------------
If the datasets have already been generated, then skip to step 5.

1) Aquire the master file (in FASTA format) that will be used to generate the subsets.
2) BLAST the master file and create the file of alignments using PISCES.
	This uses the modified Cull_for_UserSEQ.pl PISCES file. Perform this step using the command:
		perl path\to\PISCES\bin\Cull_for_UserSEQ.pl -i path\to\FASTA\file
	As an example, assume that the FASTA file is called Human.fasta, and that the file is located at C:\Fasta\Human.fasta.
	Then the path\to\FASTA\file used would be C:\Fasta\Human, the '.fasta' at the end is removed. For any FASTA file submitted you should remove the file extension, irrespective of the actual extension.
3) Rename the pdbaa.align file created in the PISCES bin directory to Aligned.txt. Move the Aligned.txt file to a directory which is not the PISCES bin directory.
4) Generate the subsets of the Human.fasta file.
	The command used for this was:
		python path\to\python\code\alignmentscreatefiles.py C:\Fasta\Human.fasta path\to\Aligned.txt 100-250-500-1000-2000-5000 50-50-50-50-50-50 path\to\results\directory
	This will generate subsets of Human.fasta of 100, 250, 500, 1000, 2000 and 5000 sequences. For each of these dataset sizes, 50 datasets will be generated.
	The sub-directories generated by this command will be called path\to\results\directory\Results100 for datasets of 100 proteins, path\to\results\directory\Results200 for datasets of 200 proteins and so on for the others.
	Refer to the Directory Structure Expected section of the Protein Subset Datasets section for information on the directory structure this command generates.

At this point the datasets have all been generated, and the comparisons between the algorithms can be run.

5) Run the algorithm comparison on the sub-datasets generated (ensure that the time limit is properly set within the code).
	The command used for this was:
		python path\to\python\code\controllersubsets.py path\to\results\directory\subdir Leaf-FIS-NeighbourCull-GLP-VSA-BlastCuller-UCLUST 1
	This command has to be run once for every sub-directory (set of sub-datasets) you want to perform the comparison on.
	Assuming steps 1 through 4 have been followed identically, then under the directory path\to\results\directory there should be six sub-directories.
	The names of these sub-directories will be Results100, Results250, Results500, Results1000, Results2000 and Results5000.
	path\to\results\directory\subdir should be one of these six sub-directories.
	This means that the command will have to be run 6 times (once with each sub-directory) if you want to compare the algorithm on all six dataset sizes.


###########################################################
Redundancy Checking
###########################################################
Directory Structure Expected
-----------------------------------------------------------
The directory structure expected is the same as the directory structure generated by the Protein Subset Datasets or Biological Datasets workflow.
As an example, a basic format for the strucutre expected is:
	path\to\results\directory
		\Dataset0
		...
		\DatasetN
		\XPercentResults.txt
		\YPercentResults.txt
		\ZPercentResults.txt

Directory Structure Generated
-----------------------------------------------------------
The directory generated contains the non-redundant datasets of the specified algorithms at all the percentage sequence identities run.
An example of the output directory structure would be:
	path\to\output\directory
		\10.txt
		\20.txt
		...
		\90.txt

Workflow
-----------------------------------------------------------
This procedure will enable you to determine the percentage of 'non-redundant' proteins that have an edge remaining in the graph.
It therefore provides an estimate of the redundancy remaining in the dataset.

1) Carry out the Protein Subset Datasets or Biological Datasets workflow.
3) Run the algorithm deficiency checking script.
	The command for this was:
		python path\to\python\code\algorithmdeficiencychecking.py path\to\Aligned.txt path\to\results\directory AlgsToUse path\to\output\directory
	path\to\Aligned.txt is the file of alignments between proteins generated in step 2 of the Protein Subset Dataset workflow.
	path\to\results\directory is the path to the input directory (the one containing the results of the Protein Subset Datasets or Biological Datasets workflow).
	AlgsToUse is thelist of the algorithms must only contain algorithms used in either the Protein Subset Datasets or the Biological Datasets algorithm comparison.
	path\to\output\directory is the path to the directory that will contain the output results of the deficiency checking.